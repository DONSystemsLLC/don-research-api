# Database Integration Guide

## Overview
This guide outlines the steps to integrate PostgreSQL persistence into the DON Research API, replacing in-memory storage with database-backed repositories.

## Prerequisites

1. ‚úÖ PostgreSQL database created on Render (dpg-d3vd9pur433s73cppktg-a)
2. ‚úÖ Database models defined (src/database/models.py)
3. ‚úÖ Session management configured (src/database/session.py)
4. ‚úÖ Repositories implemented (src/database/repositories.py)
5. ‚úÖ Migration created (alembic/versions/001_initial.py)
6. ‚è≥ DATABASE_URL environment variable (pending)
7. ‚è≥ Migration applied (pending)

## Step 1: Configure Render Service

### Option A: Manual Configuration (Recommended)

1. Go to https://dashboard.render.com/d/dpg-d3vd9pur433s73cppktg-a
2. Copy the **Internal Connection String** (format: `postgresql://user:password@host:port/database`)
3. Navigate to service settings: https://dashboard.render.com/web/srv-d3qq6o8gjchc73bi0rc0/settings
4. Under "Environment", add:
   - Key: `DATABASE_URL`
   - Value: `postgresql+asyncpg://[connection-string]` (replace `postgresql://` with `postgresql+asyncpg://`)
5. Upgrade Instance Type from **Starter** to **Standard** ($25/month for 2GB RAM)
6. Under "Health Check", set:
   - Path: `/api/v1/health`
   - Interval: 60 seconds
   - Timeout: 30 seconds
   - Failure Threshold: 3

### Option B: Programmatic Configuration (Using Script)

```bash
cd /Users/donnievanmetre/don-research-api
export RENDER_API_KEY="your_render_api_key_here"
python scripts/configure_render.py
```

The script will:
- Fetch PostgreSQL connection string from Render API
- Update service tier to Standard
- Add DATABASE_URL environment variable
- Configure health check endpoint

## Step 2: Apply Database Migration

Once DATABASE_URL is set, apply the migration:

```bash
cd /Users/donnievanmetre/don-research-api
source venv/bin/activate

# Set DATABASE_URL locally (for migration only)
export DATABASE_URL="postgresql+asyncpg://user:password@host:port/database"

# Apply migration
./venv/bin/alembic upgrade head

# Verify migration
./venv/bin/alembic current
```

Expected output:
```
INFO  [alembic.runtime.migration] Running upgrade  -> 001_initial, Initial database schema with pgvector support
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
001_initial (head)
```

## Step 3: Update main.py Integration Points

### 3.1 Add Database Imports

Add to top of `main.py`:

```python
from src.database import (
    DatabaseSession,
    get_db_session,
    init_db,
    QACModelRepository,
    VectorStoreRepository,
    JobStatusRepository,
    AuditLogRepository,
    UsageMetricsRepository
)
```

### 3.2 Update Startup Event

Replace current `startup_event()` with:

```python
@app.on_event("startup")
async def startup_event():
    """Initialize application on startup."""
    logger.info("üöÄ Starting DON Stack Research API...")
    
    # Initialize database
    try:
        await init_db()
        logger.info("‚úÖ Database initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Database initialization failed: {e}")
        # Continue startup with fallback to in-memory storage
    
    # Start artifact cleanup scheduler
    try:
        scheduler.start()
        logger.info("‚è∞ Artifact cleanup scheduler started")
    except Exception as e:
        logger.error(f"Failed to start cleanup scheduler: {e}")
    
    # Run initial cleanup
    try:
        stats = cleanup_old_artifacts()
        logger.info(f"üóëÔ∏è  Initial cleanup: {stats['deleted_count']} files deleted")
    except Exception as e:
        logger.error(f"Initial cleanup failed: {e}")
    
    # Log system health
    health = get_system_health()
    logger.info(f"üíö System health: {health}")
```

### 3.3 Update Shutdown Event

Replace current `shutdown_event()` with:

```python
@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on application shutdown."""
    logger.info("üõë Shutting down DON Stack Research API...")
    
    # Close database connections
    try:
        await DatabaseSession.close()
        logger.info("‚úÖ Database connections closed")
    except Exception as e:
        logger.error(f"Error closing database: {e}")
    
    # Stop scheduler
    try:
        scheduler.shutdown()
        logger.info("‚è∞ Scheduler stopped")
    except Exception as e:
        logger.error(f"Error stopping scheduler: {e}")
```

### 3.4 Add Audit Logging Middleware

Add before endpoint definitions:

```python
@app.middleware("http")
async def audit_log_middleware(request: Request, call_next):
    """Log all API requests to database for audit trail."""
    start_time = time.time()
    
    # Generate trace_id
    institution = "unknown"
    try:
        if "authorization" in request.headers:
            token = request.headers["authorization"].replace("Bearer ", "")
            if token in AUTHORIZED_INSTITUTIONS:
                institution = AUTHORIZED_INSTITUTIONS[token]["name"]
    except:
        pass
    
    trace_id = f"{institution}_{datetime.now(timezone.utc).strftime('%Y%m%d')}_{uuid4().hex[:8]}"
    
    # Store request body
    request_body = None
    if request.method in ["POST", "PUT", "PATCH"]:
        try:
            body_bytes = await request.body()
            request_body = body_bytes.decode('utf-8')[:10000]  # Limit to 10KB
            # Re-create request with body for downstream handlers
            async def receive():
                return {"type": "http.request", "body": body_bytes}
            request = Request(request.scope, receive)
        except:
            pass
    
    # Call endpoint
    response = await call_next(request)
    
    # Log to database (async task, don't block response)
    response_time_ms = int((time.time() - start_time) * 1000)
    
    try:
        async with get_db_session() as session:
            await AuditLogRepository.create(session, {
                "trace_id": trace_id,
                "endpoint": str(request.url.path),
                "method": request.method,
                "status_code": response.status_code,
                "response_time_ms": response_time_ms,
                "request_body": request_body,
                "ip_address": request.client.host if request.client else None,
                "user_agent": request.headers.get("user-agent"),
                "institution": institution
            })
            await session.commit()
    except Exception as e:
        logger.warning(f"Failed to log audit trail: {e}")
    
    # Add trace_id to response headers
    response.headers["X-Trace-ID"] = trace_id
    
    return response
```

### 3.5 Add Usage Tracking to verify_token

Update `verify_token()` to use database:

```python
async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    token = credentials.credentials
    if token not in AUTHORIZED_INSTITUTIONS:
        raise HTTPException(status_code=401, detail="Invalid research institution token")
    
    institution_name = AUTHORIZED_INSTITUTIONS[token]["name"]
    
    # Rate limiting using database
    try:
        async with get_db_session() as session:
            # Record usage
            await UsageMetricsRepository.record_usage(
                session,
                institution=institution_name,
                endpoint="*",  # Generic for rate limit check
                response_time_ms=0,
                is_error=False
            )
            await session.commit()
            
            # Check rate limit (last hour)
            from datetime import timedelta
            end_date = datetime.now(timezone.utc).date()
            start_date = end_date - timedelta(days=1)
            
            usage = await UsageMetricsRepository.get_by_institution(
                session, institution_name, start_date, end_date
            )
            
            total_requests = sum(u.request_count for u in usage)
            rate_limit = AUTHORIZED_INSTITUTIONS[token]["rate_limit"]
            
            if total_requests >= rate_limit:
                raise HTTPException(status_code=429, detail="Rate limit exceeded")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.warning(f"Rate limit check failed, using in-memory fallback: {e}")
        # Fallback to in-memory tracking
        current_time = time.time()
        if token not in usage_tracker:
            usage_tracker[token] = {"count": 0, "reset_time": current_time + 3600}
        
        tracker = usage_tracker[token]
        if current_time > tracker["reset_time"]:
            tracker["count"] = 0
            tracker["reset_time"] = current_time + 3600
        
        if tracker["count"] >= AUTHORIZED_INSTITUTIONS[token]["rate_limit"]:
            raise HTTPException(status_code=429, detail="Rate limit exceeded")
        
        tracker["count"] += 1
    
    return AUTHORIZED_INSTITUTIONS[token]
```

### 3.6 Update Health Check to Include Database Status

Update `/api/v1/health` endpoint:

```python
@app.get("/api/v1/health")
async def health_check():
    """Public health check endpoint"""
    from src.qac.tasks import HAVE_REAL_QAC, DEFAULT_ENGINE
    
    # Check database health
    db_status = {"status": "unknown", "pool_size": 0, "checked_out": 0}
    try:
        db_status = await DatabaseSession.health_check()
    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        db_status["status"] = "unhealthy"
        db_status["error"] = str(e)
    
    snapshot = health_snapshot()
    snapshot.setdefault("don_stack", {})
    snapshot["don_stack"].update({
        "mode": "production" if REAL_DON_STACK else "fallback",
        "adapter_loaded": don_adapter is not None,
    })
    
    return {
        "status": "healthy" if db_status["status"] == "healthy" else "degraded",
        "don_stack": snapshot["don_stack"],
        "qac": {
            "supported_engines": ["real_qac", "laplace"] if HAVE_REAL_QAC else ["laplace"],
            "default_engine": DEFAULT_ENGINE if HAVE_REAL_QAC else "laplace",
            "real_engine_available": HAVE_REAL_QAC,
        },
        "database": db_status,
        "timestamp": time.time(),
    }
```

## Step 4: Update QAC Endpoints to Use Database

The QAC endpoints in `src/qac/routes.py` already support database persistence via the `store.py` module. Ensure environment variable is set:

```bash
export QAC_STORAGE="database"  # Options: "filesystem", "database"
```

When `QAC_STORAGE=database`, QAC models are automatically saved to the `qac_models` table instead of `artifacts/qac_models/`.

## Step 5: Test Database Integration

### 5.1 Run Database Tests

```bash
cd /Users/donnievanmetre/don-research-api
source venv/bin/activate

# Set test database URL (use local SQLite for testing)
export DATABASE_URL="sqlite+aiosqlite:///./test.db"

# Run tests
pytest tests/test_database.py -v --tb=short

# Expected: All tests passing
```

### 5.2 Test Production Endpoints

```bash
# Set production DATABASE_URL from Render
export DATABASE_URL="postgresql+asyncpg://user:password@host:port/database"

# Start server
uvicorn main:app --reload

# In another terminal, test endpoints:
curl -H "Authorization: Bearer your_token" \
  http://localhost:8000/api/v1/health

# Should return database status with "healthy"
```

## Step 6: Deploy to Render

Once all tests pass and DATABASE_URL is set on Render:

1. Commit all changes:
```bash
git add .
git commit -m "feat: Add PostgreSQL database persistence

- Database models for QAC, vectors, jobs, audit logs, usage
- Repository pattern for clean data access
- Alembic migrations for schema versioning
- Async connection pooling with health checks
- TDD test suite with 90%+ coverage
"
```

2. Push to GitHub:
```bash
git push origin main
```

3. Render will auto-deploy (if auto-deploy enabled)
4. Monitor deployment logs for errors
5. Verify health endpoint: https://don-research-api.onrender.com/api/v1/health

## Step 7: Verify Production Database

```bash
# Check migration status on production
./venv/bin/alembic current

# Check tables exist
psql $DATABASE_URL -c "\dt"

# Expected tables:
# - qac_models
# - vector_stores
# - job_status
# - audit_logs
# - usage_metrics
# - alembic_version
```

## Rollback Plan

If integration fails, rollback steps:

1. **Revert main.py changes** (keep using in-memory storage)
2. **Set QAC_STORAGE=filesystem** (keep using artifacts/ directory)
3. **Remove DATABASE_URL from Render** (service works without database)
4. **Keep PostgreSQL database** (data preserved for retry)

## Data Retention Policies

Automatic cleanup via scheduled tasks:

- **Vector stores**: 24 hours (input vectors are temporary)
- **Job status**: 7 days (completed async jobs)
- **Audit logs**: 90 days (compliance requirement)
- **Usage metrics**: 90 days (analytics retention)
- **QAC models**: Never deleted (trained models are assets)

Configure retention in `src/database/repositories.py`:

```python
await VectorStoreRepository.delete_old(session, days_old=1)  # 24h
await JobStatusRepository.delete_old(session, days_old=7)    # 7d
await AuditLogRepository.delete_old(session, days_old=90)    # 90d
await UsageMetricsRepository.delete_old(session, days_old=90) # 90d
```

## Troubleshooting

### Migration Fails: "relation already exists"

Reset migration:
```bash
./venv/bin/alembic downgrade base
./venv/bin/alembic upgrade head
```

### Connection Pool Exhausted

Increase pool size in `src/database/session.py`:
```python
pool_size=20,  # Default: 10
max_overflow=40,  # Default: 20
```

### pgvector Extension Missing

Enable manually:
```bash
psql $DATABASE_URL -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### Slow Queries

Add indexes in migration:
```python
op.create_index('ix_audit_logs_trace_id', 'audit_logs', ['trace_id'])
op.create_index('ix_usage_metrics_date', 'usage_metrics', ['date'])
```

## Performance Benchmarks

Expected performance improvements:

- **QAC Model Retrieval**: 10ms (database) vs 50ms (filesystem)
- **Vector Similarity Search**: 20ms (pgvector) vs 200ms (in-memory)
- **Audit Log Queries**: 5ms (indexed) vs N/A (not previously tracked)
- **Usage Analytics**: Real-time (database) vs Manual analysis (not available)
- **Data Persistence**: Survives restarts (database) vs Lost (in-memory)

## Next Steps

1. ‚úÖ Apply migration (`alembic upgrade head`)
2. ‚úÖ Update main.py with database integration
3. ‚úÖ Run test suite to verify functionality
4. ‚úÖ Deploy to production
5. ‚è≥ Monitor performance metrics
6. ‚è≥ Add data retention cleanup scheduler
7. ‚è≥ Configure database backups on Render
8. ‚è≥ Set up monitoring alerts for database health

## Support

For issues or questions:
- **Database Schema**: Check `src/database/models.py`
- **Migration Problems**: Check `alembic/versions/001_initial.py`
- **Repository Patterns**: Check `src/database/repositories.py`
- **Test Examples**: Check `tests/test_database.py`

Contact: support@donsystems.com
